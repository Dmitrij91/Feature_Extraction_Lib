{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8aceae2-86ae-4be2-9fc4-bc22ac523bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cac5b9d9-6240-4615-b04a-599be9ca5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model': 'ResNet',\n",
    " 'model_size': 8,\n",
    " 'activation': 'relu',\n",
    " 'window_size': {'x': 10, 'y': 10},\n",
    " 'dropout': 0.0,\n",
    " 'workers_training': 4,\n",
    " 'workers_testing': 4,\n",
    " 'machine': 'barracuda',\n",
    " 'batch_size': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c1ee302-2043-4fc0-b91b-c1bee1c5e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_window = (config['window_size']['y'], config['window_size']['x'])\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46635c-73c0-45b3-8402-255f5bd8c5ee",
   "metadata": {},
   "source": [
    "# Import Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbf709b0-f21a-4ba0-ad85-cb408bedad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataclass\n",
    "sys.path.append('../src/Deep_Features/')\n",
    "from dataloader_2d import build_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6416305-0cdc-4dd7-ad9a-119c862fbf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08a74848-8754-40f8-ace9-6aeb35937efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)  \n",
    "\n",
    "\n",
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,Features = False, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.Features = Features\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "\n",
    "\n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "\n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "\n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )\n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, blocks_sizes=[64, 128, 256, 512], depths=[2,2,2,2], \n",
    "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=depths[0], activation=activation, \n",
    "                        block=block,*args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, depths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes,Features, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        if Features:\n",
    "            self.decoder = nn.Sequential(nn.Dropout(p=1.0-dropout),\n",
    "                nn.Identity(in_features)\n",
    "                                        )   \n",
    "        else:\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Dropout(p=1.0-dropout),\n",
    "                nn.Linear(in_features, n_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes,Features, dropout=0.0, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.Features = Features\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes,Features, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def resnet(size, in_channels, n_classes,Feat, *args, **kwargs):\n",
    "    layer_depths = {\n",
    "        5:   [1, 1, 1, 1],\n",
    "        6:   [1, 1, 1, 1],\n",
    "        7:   [1, 1, 1, 1],\n",
    "        8:   [1, 1, 1, 1],\n",
    "        9:   [1, 1, 1, 1],\n",
    "        10:  [1, 1, 1, 1],\n",
    "        18:  [2, 2, 2, 2], \n",
    "        34:  [3, 4, 6, 3],\n",
    "        50:  [3, 4, 6, 3],\n",
    "        101: [3, 4, 23, 3],\n",
    "        152: [3, 8, 36, 3]\n",
    "    }\n",
    "    blocks = {\n",
    "        5:   ResNetBasicBlock,\n",
    "        6:   ResNetBasicBlock,\n",
    "        7:   ResNetBasicBlock,\n",
    "        8:   ResNetBasicBlock,\n",
    "        9:   ResNetBasicBlock,\n",
    "        10:  ResNetBasicBlock,\n",
    "        18:  ResNetBasicBlock,\n",
    "        34:  ResNetBasicBlock,\n",
    "        50:  ResNetBottleNeckBlock,\n",
    "        101: ResNetBottleNeckBlock,\n",
    "        152: ResNetBottleNeckBlock\n",
    "    }\n",
    "    channel_dims = {\n",
    "        5:   [2, 4, 8, 16],\n",
    "        6:   [4, 8, 16, 32],\n",
    "        7:   [8, 16, 32, 64],\n",
    "        8:   [16, 32, 64, 128],\n",
    "        9:   [32, 64, 128, 256],\n",
    "        10:  [64, 128, 256, 512],\n",
    "        18:  [64, 128, 256, 512],\n",
    "        34:  [64, 128, 256, 512],\n",
    "        50:  [64, 128, 256, 512],\n",
    "        101: [64, 128, 256, 512],\n",
    "        152: [64, 128, 256, 512]\n",
    "    }\n",
    "    assert layer_depths.keys() == blocks.keys()\n",
    "    assert size in layer_depths.keys()\n",
    "    return ResNet(in_channels, n_classes,Features = Feat, block=blocks[size], blocks_sizes=channel_dims[size], depths=layer_depths[size], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bcf01550-981a-45c0-8573-86720fa51ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import path to directory folder \n",
    "path = os.path.split(os.getcwd())[0]+'/src/Deep_Features'\n",
    "#path = os.path.join(path,'/src/Deep_Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f93a2e13-4c3c-4444-a544-18d607e566e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Train = pd.DataFrame()\n",
    "' Splitt the data Set into two sets for training and for testing '\n",
    "\n",
    "Ind = np.random.choice(327,1)\n",
    "Train_Paths = []\n",
    "Train_Paths_Labels = []\n",
    "Num_List = []\n",
    "\n",
    "for im_num in Ind:\n",
    "    Num_List.append(im_num)\n",
    "    if len(str(im_num)) == 3:\n",
    "        path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse'+str(im_num)+\".png\"\n",
    "        path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse'+str(im_num)+\".png\"\n",
    "        Train_Paths.append(path_1)\n",
    "        Train_Paths_Labels.append(path_2)\n",
    "    elif len(str(im_num)) == 2:\n",
    "        path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse0'+str(im_num)+\".png\"\n",
    "        path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse0'+str(im_num)+\".png\"\n",
    "        Train_Paths.append(path_1)\n",
    "        Train_Paths_Labels.append(path_2)\n",
    "    elif len(str(im_num)) == 1 and im_num != 0:\n",
    "        path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse00'+str(im_num)+\".png\"\n",
    "        path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse00'+str(im_num)+\".png\"\n",
    "        Train_Paths.append(path_1)\n",
    "        Train_Paths_Labels.append(path_2)\n",
    "\n",
    "' Train_Set '\n",
    "D_Train = pd.DataFrame({'Train_Path':Train_Paths,'Train_Path_Labels':Train_Paths_Labels})\n",
    "\n",
    "' Test_Set '\n",
    "Test_Paths = []\n",
    "Test_Paths_Labels = []\n",
    "for k in range(1,327):\n",
    "    if k not in Num_List:\n",
    "        if len(str(k)) == 3:\n",
    "            path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse'+str(k)+\".png\"\n",
    "            path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse'+str(k)+\".png\"\n",
    "            Test_Paths.append(path_1)\n",
    "            Test_Paths_Labels.append(path_2)\n",
    "        elif len(str(k)) == 2:\n",
    "            path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse0'+str(k)+\".png\"\n",
    "            path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse0'+str(k)+\".png\"\n",
    "            Test_Paths.append(path_1)\n",
    "            Test_Paths_Labels.append(path_2)\n",
    "        elif len(str(k)) == 1 and im_num != 0:\n",
    "            path_1 = '../../FM_Eikonal/data/weizmann_horse_db/horse/horse00'+str(k)+\".png\"\n",
    "            path_2 = '../../FM_Eikonal/data/weizmann_horse_db/mask//horse00'+str(k)+\".png\"\n",
    "            Test_Paths.append(path_1)\n",
    "            Test_Paths_Labels.append(path_2)\n",
    "        if k >1:\n",
    "            break\n",
    "        \n",
    "D_Test = pd.DataFrame({'Test_Path':Test_Paths,'Test_Path_Labels':Test_Paths_Labels})\n",
    "\n",
    "D_Train.to_csv('../src/Deep_Features/training_data.csv')\n",
    "D_Test.to_csv('../src/Deep_Features/testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad6bb278-cdcc-4b8d-b774-ea40a30bc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2\n",
    "model = resnet(config['model_size'], 1, c, dropout=config['dropout'],Feat=False)\n",
    "size_window = (config['window_size']['x'],config['window_size']['y'])\n",
    "\n",
    "fully_convolutional = (config['model'] in ['ResNet'])\n",
    "model.to(device)\n",
    "\n",
    "train_dataloader = build_data_loader(size_window, c,path_dir=path,\n",
    "    train=True,\n",
    "    batch_size=config['batch_size'],\n",
    "    workers=config['workers_training'],\n",
    "    single_pixel_out=(not fully_convolutional))\n",
    "\n",
    "test_dataloader = build_data_loader(\n",
    "    size_window, c,path_dir=path,\n",
    "    train=False,\n",
    "    batch_size=config['batch_size'],\n",
    "    workers=config['workers_testing'],\n",
    "    single_pixel_out=(not fully_convolutional))\n",
    "test_iter = iter(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "934907ee-5660-4233-8b2b-e58c6a69ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa813e08-79bb-4d19-966d-971c3ff2d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 999 training loss 0.5501869230866432\n",
      "batch 999 testing loss 0.6561069503426552\n",
      "batch 1999 training loss 0.5504361751973629\n",
      "batch 1999 testing loss 0.5772193163633347\n",
      "batch 2999 training loss 0.549860578417778\n",
      "batch 2999 testing loss 0.5112241372466088\n",
      "batch 3999 training loss 0.542431331127882\n",
      "batch 3999 testing loss 0.5626655921339989\n",
      "batch 4999 training loss 0.5443222447931767\n",
      "batch 4999 testing loss 0.5520967155694961\n",
      "batch 5999 training loss 0.5475694245398045\n",
      "batch 5999 testing loss 0.6218102470040321\n",
      "batch 6999 training loss 0.5441385338306427\n",
      "batch 6999 testing loss 0.5439687550067902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fully_convolutional:\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;66;03m# add channel dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/queues.py:116\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:495\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 495\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/resource_sharer.py:87\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     86\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 87\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/connection.py:502\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    500\u001b[0m     c \u001b[38;5;241m=\u001b[39m PipeClient(address)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mSocketClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/connection.py:629\u001b[0m, in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    627\u001b[0m family \u001b[38;5;241m=\u001b[39m address_type(address)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m socket\u001b[38;5;241m.\u001b[39msocket( \u001b[38;5;28mgetattr\u001b[39m(socket, family) ) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m--> 629\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetblocking\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    630\u001b[0m     s\u001b[38;5;241m.\u001b[39mconnect(address)\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Connection(s\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "log_interval = int(1e3)\n",
    "save_interval = int(1e4)\n",
    "num_test_batches = 20\n",
    "\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if fully_convolutional:\n",
    "            # add channel dimension\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        else:\n",
    "            labels = labels.flatten()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        if fully_convolutional:\n",
    "            # remove channel dimension\n",
    "            outputs = torch.squeeze(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels[:,0,0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % log_interval == log_interval-1:\n",
    "            print(f\"batch {i} training loss {running_loss / log_interval}\")\n",
    "            running_loss = 0.0\n",
    "            # run testing\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            for test_batches in range(num_test_batches):\n",
    "                try:\n",
    "                    test_data, test_labels = next(test_iter)\n",
    "                except StopIteration:\n",
    "                    test_iter = iter(test_dataloader)\n",
    "                    test_data, test_labels = next(test_iter)\n",
    "                if fully_convolutional:\n",
    "                    test_data = test_data.unsqueeze(1)\n",
    "                else:\n",
    "                    test_labels = test_labels.flatten()\n",
    "                test_data = test_data.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "                test_out = model(test_data)\n",
    "                if fully_convolutional:\n",
    "                    test_out = torch.squeeze(test_out, 1)\n",
    "                test_loss += criterion(test_out, test_labels[:,0,0]).item()\n",
    "            print(f\"batch {i} testing loss {test_loss / num_test_batches}\")\n",
    "            model.train()\n",
    "\n",
    "    print(\"Finished epoch. Saving model...\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a00176ff-fe1e-404f-8883-c8fd06ecb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((64,1,40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e496c29-33f5-4b4a-bc4a-c2b21c8ea582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m resnet(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, c, dropout\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],Feat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m summary(model,\u001b[43mA\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = resnet(config['model_size'], 1, c, dropout=config['dropout'],Feat=False)\n",
    "summary(model,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d341d1a-6897-4ebd-9f22-6a3c3a97bbea",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.299\u001b[39m,\u001b[38;5;241m0.587\u001b[39m,\u001b[38;5;241m0.114\u001b[39m])\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m255.0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m vol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mijk,k->ij\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mD_Train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain_Path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mcopy())\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mdouble)[:,:,:],mask)\n\u001b[1;32m      8\u001b[0m window_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      9\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage.util import view_as_blocks,view_as_windows\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array([0.299,0.587,0.114])/(255.0)\n",
    "vol = np.einsum('ijk,k->ij',np.array(Image.open(D_Train['Train_Path'][1]).copy()).astype(np.double)[:,:,:],mask)\n",
    "\n",
    "window_size = (10,10)\n",
    "batch_size = config['batch_size']\n",
    "overlap_windows = 3\n",
    "model.eval()\n",
    "if fully_convolutional:\n",
    "    assert 2*overlap_windows < min(*window_size)\n",
    "    inner_window_size = tuple([ws-2*overlap_windows for ws in window_size])\n",
    "    # crop vol to make it compatible with windowig stride\n",
    "    block_count = [(vs - 2*overlap_windows) // iws for vs, iws in zip(vol.shape, inner_window_size)]\n",
    "    vol_shape = tuple([b*iws + 2*overlap_windows for b, iws in zip(block_count, inner_window_size)])\n",
    "    vol = vol[:vol_shape[0],:vol_shape[1]]\n",
    "    \n",
    "    # gather windows for prediction\n",
    "    patches = view_as_windows(vol, window_size, step=inner_window_size).reshape((-1,*window_size))\n",
    "    cropped_vol_shape = tuple([vs - 2*overlap_windows for vs in vol.shape])\n",
    "    scores = np.empty((*cropped_vol_shape, c), dtype=np.float32)\n",
    "    scores_view = view_as_blocks(scores, (*inner_window_size, c))\n",
    "    \n",
    "    for batch_ind in tqdm(range(1 + patches.shape[0] // batch_size)):\n",
    "        batch_input = torch.FloatTensor(patches[batch_ind*batch_size:(batch_ind+1)*batch_size,...]).to(device)\n",
    "        if batch_input.shape[0] == 0:\n",
    "            continue\n",
    "        predictions = model(batch_input[:,None,:,:]).data.cpu().numpy()\n",
    "\n",
    "        for i, ind in enumerate(range(batch_ind*batch_size, (batch_ind+1)*batch_size)):\n",
    "            if i < predictions.shape[0]:\n",
    "                block_ind = np.unravel_index(ind, scores_view.shape[:2])\n",
    "                scores_view[block_ind[0],block_ind[1],0,...] = predictions[i,:]\n",
    "\n",
    "dist = - scores\n",
    "dist -= dist.min()\n",
    "dist /= dist.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2baefb09-0806-462a-8f15-b6b6ab459fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 707.5, 423.5, -0.5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAE9CAYAAACWQ2EXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFVUlEQVR4nO3YsRGDUAwFQcO4NUqgSpfg3iyXAAnzg9uNFbzwRtvMzAsAyNpXDwAA1hIDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiHvfPTz288kdAMADvr/P5Y3PAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOK2mZnVIwCAdXwGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACDuD/ZsDXOsXTvaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.argmin(dist,axis=2))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b2939a1e-5bb8-4788-9a6c-7ae7069fc2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        ...,\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ]],\n",
       "\n",
       "       [[0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        ...,\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ]],\n",
       "\n",
       "       [[0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        [0.9528749 , 0.30140358],\n",
       "        ...,\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ],\n",
       "        [0.8423333 , 0.2828602 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        ...,\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ]],\n",
       "\n",
       "       [[0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        ...,\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ]],\n",
       "\n",
       "       [[0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        [0.53694654, 0.14748013],\n",
       "        ...,\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ],\n",
       "        [0.43427294, 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f97ad-e1a1-482b-980c-29d31016fe84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6045c98-252e-4448-b6ff-6e507c0ce7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9695fee1-0e23-41c9-9cea-f8e7f901b2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638747ed-c3dc-4a92-aec7-b8791d0fa9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e798b3ad-c77e-4e9e-a82a-6ee2aa4e16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abea5874-3f97-49b1-9d7d-936886c9a4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ../../FM_Eikonal/data/weizmann_horse_db/horse/...\n",
       "1    ../../FM_Eikonal/data/weizmann_horse_db/horse/...\n",
       "Name: Test_Path, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_Test['Test_Path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b99d16-ae36-431b-9c85-369c8c7a9b22",
   "metadata": {},
   "source": [
    "# UNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "7bb52bb6-b194-4453-abc1-b2dd701f9989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0024, -0.0931],\n",
       "        [ 0.0438, -0.1351],\n",
       "        [-0.1378, -0.0772],\n",
       "        [-0.0529, -0.0639],\n",
       "        [ 0.0286, -0.0627],\n",
       "        [ 0.0005, -0.0141],\n",
       "        [-0.0656, -0.0208],\n",
       "        [-0.0633, -0.0611],\n",
       "        [ 0.1784, -0.1338],\n",
       "        [ 0.0134, -0.0677],\n",
       "        [ 0.0254, -0.0833],\n",
       "        [ 0.0094, -0.0410],\n",
       "        [-0.0277, -0.0627],\n",
       "        [ 0.0997, -0.0294],\n",
       "        [-0.0303, -0.0352],\n",
       "        [ 0.0375, -0.0977],\n",
       "        [-0.0243, -0.0341],\n",
       "        [-0.0653, -0.0612],\n",
       "        [-0.0384, -0.0383],\n",
       "        [ 0.0147, -0.1102],\n",
       "        [ 0.0523, -0.0807],\n",
       "        [-0.0008, -0.0241],\n",
       "        [-0.0323, -0.1087],\n",
       "        [-0.0810, -0.0213],\n",
       "        [ 0.0399, -0.0621],\n",
       "        [ 0.0153, -0.0074],\n",
       "        [ 0.0128, -0.0829],\n",
       "        [-0.0064, -0.0492],\n",
       "        [-0.0137, -0.0232],\n",
       "        [ 0.0066, -0.0842],\n",
       "        [-0.0165, -0.0823],\n",
       "        [ 0.0158, -0.1204]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "d7bc7286-70ec-44b3-92fb-7f09be077675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu(data[0][:,0,0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "63caa33e-fdd6-4a7e-9cdd-e3b86cd49f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.ReLU()\n",
    "input = torch.randn(2).unsqueeze(0)\n",
    "output = torch.cat((m(input), m(-input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "8b17270e-a020-4bf6-82f6-124786f15d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3380, -0.1105]])"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "6a65324c-b9f6-4dcc-a7ab-fdd2089236ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3380, 0.0000])"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "035dc919-222d-4e92-9e62-4d34fbbb8571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse058\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse058\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse187\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse187\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse319\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse319\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse309\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse309\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse126\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse126\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse113\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse113\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse004\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse004\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse300\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse300\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse246\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse246\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse317\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse317\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse175\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse175\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse253\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse253\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse064\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse064\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse004\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse004\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse054\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse054\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse115\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse115\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse280\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse280\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse079\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse079\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse240\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse240\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse282\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse282\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse140\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse140\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse131\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse131\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse067\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse067\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse127\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse127\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse136\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse136\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse252\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse252\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse128\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse128\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse192\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse192\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse078\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse078\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse085\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse085\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse315\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse315\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse078\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse078\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse050\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse050\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse133\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse133\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse239\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse239\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse221\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse221\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse317\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse317\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse235\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse235\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse242\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse242\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse097\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse097\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse123\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse123\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse318\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse318\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse234\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse234\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse210\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse210\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse188\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse188\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse092\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse092\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse214\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse214\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse020\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse020\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse280\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse280\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse065\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse065\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse066\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse066\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse202\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse202\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse179\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse179\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse117\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse117\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse139\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse139\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse127\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse127\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse058\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse058\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse272\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse272\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse208\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse208\n",
      "The training data path is ../../FM_Eikonal/data/weizmann_horse_db/horse/horse185\n",
      "The training data label path is ../../FM_Eikonal/data/weizmann_horse_db/mask//horse185\n"
     ]
    }
   ],
   "source": [
    "for path_train_im,path_label_im in dset_files[1:]:\n",
    "    print(f\"The training data path is {path_train_im[:-4]}\")\n",
    "    print(f\"The training data label path is {path_label_im[:-4]}\")\n",
    "    assert path_train_im.endswith(\".png\")\n",
    "    path_train_im = rel_path[1][:-4]\n",
    "    path_label_im = rel_path[2][:-4]\n",
    "    self.dsets.append((path_train_im, path_label_im, size_window, c, single_voxel_out=single_voxel_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "f0f2d37f-aa93-40fd-afd8-41dd1f62dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sequences = {\n",
    "\t\t0: [1, 64, 128, 256, 512, 1024],\n",
    "\t\t1: [1,  2,   4,   8,  16,   32],\n",
    "\t\t2: [1,  4,   8,  16,  32,   64],\n",
    "\t\t3: [1,  8,  16,  32,  64,  128],\n",
    "\t\t4: [1, 16,  32,  64, 128,  256],\n",
    "\t\t5: [1, 32,  64, 128, 256,  512]\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "3a7f19d0-6475-44d9-9110-dfed48962d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[1, 64, 128, 256, 512, 1024], [1, 2, 4, 8, 16, 32], [1, 4, 8, 16, 32, 64], [1, 8, 16, 32, 64, 128], [1, 16, 32, 64, 128, 256], [1, 32, 64, 128, 256, 512]])"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100acd5-1368-4da2-83de-7f372971c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
